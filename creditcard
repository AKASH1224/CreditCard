#!/usr/bin/env python
# coding: utf-8

# In[18]:

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix , accuracy_score, classification_report


# In[19]:


# Step 2 :-Load the dataset


# In[20]:



credit_card_data = pd.read_csv('creditcard.csv')


# In[21]:


credit_card_data.head(5)


# In[ ]:





# In[22]:


credit_card_data.tail(6)


# In[23]:


#dataset information
credit_card_data.info()


# In[24]:


#checking number of missing values:
credit_card_data.isnull().sum()
#Find distribution of normal transaction or fraud transaction 
credit_card_data['Class'].value_counts()


# In[25]:


#Now we will separate Normal and Fraud transactions, and analyze and visualize that fraud and normal data :


# In[ ]:


#Seprating the data
normal =credit_card_data[credit_card_data.Class==0]
fraud =credit_card_data[credit_card_data.Class==1]
#check shape
print("normal shape :-",normal.shape)
print("fraud.shape:-",fraud.shape)
#visualize the data:
labels =["Normal","Fraud"]
count_classes=credit_card_data.value_counts(credit_card_data['Class'],sort=True)
count_classes.plot(kind = "bar", rot = 0)
plt.title("CreditCard")
plt.ylabel("Count")
plt.xticks(range(2), labels)
plt.show()


# In[ ]:


#WE will perform  some statistical analysis of the data:


# In[ ]:


normal.Amount.describe()
fraud.Amount.describe()
# visualize the data using seaborn:
sns.relplot(x = 'Amount' , y = 'Time' , hue = 'Class', data = credit_card_data)


# In[31]:


# Compare values of both transactions:
credit_card_data.groupby('Class').mean()
# Now we will build a sample dataset containing similar distribution of normal transaction and fraud transaction:
normal_sample = normal.sample(n=492)
# Concat two data ( normal_sample and fraud) to create new dataframe which consist equal number of fraud transactions and normal transactions, In this way we balance our dataset (As our dataset is highly unbalanced initially) :
credit_card_new_data = pd.concat([normal_sample, fraud], axis=0)
#Letâ€™s see our new dataset:
credit_card_new_data
# Analyse our new dataset:
credit_card_new_data['Class'].value_counts()


# #Splitting the data:- After analyzing and visualizing our data, now we will split our dataset in X and Y or say in features and labels:

# In[33]:


# Splitting data into features and targets
X = credit_card_new_data.drop('Class', axis=1)
Y = credit_card_new_data['Class']
# splitting the data into training and testing data:
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, stratify = Y, random_state= 2)
print(X.shape, X_train.shape, X_test.shape)


# 5.) Creating Logistic Regression Model:
# now we we will create ml model

# In[34]:


# Creating Model:
model = LogisticRegression()
# training the Logistic Regression model with training data:
model.fit(X_train,Y_train)


# In[ ]:


#5.Model Evaluation:


# In[35]:


# Model Evaluation
X_train_pred = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_pred, Y_train)
print('Accuracy of Training data:', training_data_accuracy)


# In[36]:


# classification report of the model on training data:
print(classification_report(X_train_pred, Y_train))


# In[37]:


# accuracy on test data:
X_test_pred = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_pred, Y_test)
print('Accuracy of Testing data:', test_data_accuracy)

# confusion matrix and classification report of test data:
print(confusion_matrix(X_test_pred, Y_test))
print(classification_report(X_test_pred, Y_test))


# Summary:-
# This is a Machine Learning credit card fraud detection project in which we have successfully created a model that can detect that the transaction made by the person is Normal or fraudulent. In this project, we learned how to perform exploratory data analysis. And also we have learned how to handle highly unbalanced datasets using sampling. Also learned about Logistic Regression and how to create a Logistic Regression model.

# print("hello world")
